# QLGS 框架复杂度分析（基于 `Method-combination/QLDGS-PSO-Elite.py`）

本说明补充了审稿人所关心的理论复杂度。我们显式给出 QLGS 框架（QLDGS-PSO-Elite）以及基准 PSO 的 Big-O 复杂度，并解释 QLGSPSO 何以能在实测中取得更低的运行时间。

## 记号

- `d`：特征空间维度，即 `xtrain.shape[1]`（`QLDGS-PSO-Elite.py:383`）。
- `N`：粒子规模 `opts['N']`。
- `T`：最大迭代次数 `opts['T']`。
- `L`：分层区间数量 `num_intervals`，默认值约为 `4·log10(d+50)+4`。
- `M`：每个区间内部的局部评估次数 `valued_num`。
- `C_eval(k)`：`Fun(...)` 在 `k` 个特征上的评估成本。`Fun` 内部需训练/验证分类器，其复杂度与特征数量成单调关系。
- `Max_FEs = N·T`：QLGS 框架与基准 PSO 共用的评价预算（`QLDGS-PSO-Elite.py:404`）。

除 `C_eval(k)` 外，其余矢量操作、排序和随机采样都至多是多项式时间，可直接用 `O(·)` 记号描述。

## 基准 PSO 的复杂度

`baseline_pso_run`（`QLDGS-PSO-Elite.py:347-379`）的核心循环是：

1. 对 `T` 次迭代执行一次完整的群体更新。
2. 每次迭代对 `N` 个粒子进行一次 `Fun` 评估（`365` 行），并执行维度为 `d` 的速度/位置更新（`374-376` 行）。

因此，基准 PSO 的总体复杂度为

```
O(T · N · (d + C_eval(d))).
```

其中 `C_eval(d)` 往往是主导项，因为在特征选择场景中 `Fun` 需要在 `d` 维特征上训练/验证分类器。

## QLGS 框架的复杂度

QLGSPSO（`fs`，`381-616` 行）延续了相同的评价预算 `Max_FEs = N·T`，但在每次评价前加入了区间分层和 Q-learning 策略。其复杂度可拆解如下。

### 1. 区间划分与初始化

- `partition_intervals` 与 `make_increasing` 只对 `L` 个区间端点做线性处理，复杂度为 `O(L)`。
- 初始化阶段对每个区间运行 `M` 次局部搜索（`430-472` 行），因此最多进行 `L·M` 次粒子更新和 `Fun` 评估。与基准一样，每次更新的代价是 `O(d + C_eval(k))`，其中 `k` 是该区间允许的特征数。
- 如果 `L·M > N`，循环会通过 `t < Max_FEs` 的条件强制截断，保证总评估次数仍然不超过 `N·T`。

### 2. Q-learning 引导的搜索

主循环（`473-602` 行）在每次分区后重复以下步骤：

1. 依据当前状态选择动作（`select_len_action` → `compute_len_action`），该过程在 4×4 的 Q 表中完成，属于 `O(1)`。
2. `compute_len_state` 对 `L` 个区间的适应度执行 `np.argsort`（`553` 行），复杂度 `O(L log L)`。
3. 对每个区间依次展开 `M` 次粒子迭代并调用 `Fun`（`489-545` 行）。每次迭代与基准 PSO 相同，仍然是 `O(d + C_eval(k))`。
4. 更新奖励与 Q 表（`558-563` 行）只涉及常数级操作。

当状态收敛或达到预算后，算法进入最后的“填满预算”阶段（`566-601` 行），该阶段对剩余评估预算重复执行步骤 3 中的粒子更新，仍受 `Max_FEs` 限制。

### 3. 总体 Big-O

综上，QLGS 框架的主导项依然来自 `Fun` 评估及 `d` 维的速度/位置更新，只是这些评估被分布到各个区间和层次。将初始化、主循环与结尾阶段合并，可得

```
O(T · N · (d + C_eval(k̄)))  +  O(T · (L log L + L)),
```

其中 `k̄` 表示在 QLGS 过程中被允许的平均特征长度，满足 `k̄ << d`（因为 `Nsfn` 的上下界由 `intervals[i]` 控制，见 `500-507` 行）。附加的 `O(T · (L log L + L))` 源自区间排序与状态评估，相比 `C_eval(k̄)` 可以忽略不计（`L` 通常小于 20）。

## 与基准 PSO 的比较

| 方法 | 主导复杂度 | 说明 |
| --- | --- | --- |
| 基准 PSO (`baseline_pso_run`) | `O(T · N · (d + C_eval(d)))` | 每轮强制评估所有粒子，分类器始终基于完整特征集合 |
| QLGSPSO (`fs`) | `O(T · N · (d + C_eval(k̄))) + O(T · (L log L + L))` | 评价预算与 PSO 相同，但通过 Q-learning 控制特征长度，使得 `k̄ << d` |

**关键结论：** 两种方法对 `T` 与 `N` 保持同阶，QLGS 额外的状态/动作更新为低阶项；而 `C_eval(k̄)` 显著小于 `C_eval(d)`，因为

1. `prune_solution_pbest` 将候选特征数裁剪到区间限制（`506-527` 行），在早期就迫使粒子评估更短的特征子集。
2. `compute_len_state` 根据区间表现迭代收缩搜索空间，快速排除高维区间，进一步降低后续 `Fun` 评估的维度。
3. 即使 `L·M` 在常数上大于 `N`，算法也严格受 `Max_FEs` 约束，因此不会增加 `Fun` 的调用次数，只是用 Q-learning 策略更高效地分配这些调用。

因此，在 Big-O 意义下，QLGS 與基准 PSO 拥有相同的最高阶项，但通过 Q 表驱动的区间缩放显著降低了 `C_eval(k̄)` 的常数，解释了实验中 “one-fifth runtime” 与 “5.8-6.3× faster” 的现象。
